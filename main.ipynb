{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPc/ZCMee48k9l8JbrJlaAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RemiCailliot/Deep_Learning_Project/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-H5PaXOb-Ww8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5ba1MPo-hmE",
        "outputId": "3b8cd61e-5c58-41f8-ec51-811fbd20876e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNHUtCtZ-oxE",
        "outputId": "bf5e6b72-197b-463e-d207-ae2742291976"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2tDx7UkAdiT",
        "outputId": "dcd5c81e-af69-4ca6-d91e-9a532ac5c2a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_6Xs7ZAAikF",
        "outputId": "7751a506-d52a-4dad-f933-989b05f9aa8f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPggih7aAlbv",
        "outputId": "03dbac59-810a-4bb4-cf59-8f25b91ab807"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "G4vH79sTA_c5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC3T30LeBBNp",
        "outputId": "6f605771-52dd-468a-a28b-455aed08b4e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "p3o5pBN3BDjn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ1aM6dMBE9g",
        "outputId": "651acc52-2d33-414b-d04b-7d063f9c6760"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjg4ws3fBGgS",
        "outputId": "7e8c135a-301c-4422-f369-b26d6a5e54fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "tTyYGA0zBIJw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHi6pgtUBKF9",
        "outputId": "49de460b-89d9-45c6-e049-c67afd8e4746"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "J2jxJptdBL4w"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKMQtAuuBNas",
        "outputId": "d710b9ab-7c85-46e8-cccc-eb79f3b6c56a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "metadata": {
        "id": "6K3f5SNxBQ6_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvf1rmarBSuq",
        "outputId": "06441253-e005-4be9-c148-fc037b2b2f1d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMh4LlJLBW6f",
        "outputId": "1039ae70-4e63-437e-8786-3650ad5f1413"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "5TVTiVMqBl-G"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2uVwq8UBnSy",
        "outputId": "a3102814-2441-48ca-cc7c-da47dafd1062"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "DNyKUagqBosV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfdjLQT8BqDo",
        "outputId": "c591a7fe-5913-4b43-e64d-c85eff2e642f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOmFxNwtBsLH",
        "outputId": "0f3a5abc-8f4c-461c-9d6f-a3c64feed042"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "eKFO4fo2BuRl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "3H3DJK9FBv_0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "2Y9ghZGpBxyf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5EuWQtvB08U",
        "outputId": "4d5ed4d0-6929-4ca9-ccb7-437122e59565"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8C9w7YqB4Hb",
        "outputId": "5aab5eb0-027c-4336-ba9a-5be097a8c9d9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukhnnscjB8pl",
        "outputId": "3dea9840-1eed-48b5-81dd-6e67d13bad1f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([39,  0, 17, 38, 64, 53, 58,  6, 65, 42, 25, 33, 54, 13, 65, 15, 47,\n",
              "       32, 54, 11, 36, 61, 54, 47,  6, 46, 65,  9, 26, 45,  8,  3,  3, 27,\n",
              "       44, 36, 30, 12, 43, 53, 25,  3, 34, 17, 65, 27,  2,  2, 27, 19, 42,\n",
              "       37, 57, 34, 18, 32, 62, 34,  6,  1, 10, 51, 44, 11, 14, 24, 47, 57,\n",
              "       17, 29,  3, 26, 37, 25, 58, 51, 47,  2,  0,  6, 55, 51, 56, 25, 49,\n",
              "       35,  9, 59, 57, 14, 17, 30, 36, 17, 40, 28, 32, 41, 37, 22])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCX0anidCAcA",
        "outputId": "083ba5f1-5955-4472-9e33-6667a54a12d8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"s the lurking serpent's mortal sting?\\nNot he that sets his foot upon her back.\\nThe smallest worm wil\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"Z[UNK]DYyns'zcLTo?zBhSo:Wvoh'gz.Mf-!!NeWQ;dnL!UDzN  NFcXrUESwU'\\n3le:AKhrDP!MXLslh [UNK]'plqLjV.trADQWDaOSbXI\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "MolPy33ODlvk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHsaSdZbDpQR",
        "outputId": "89556797-45b8-4856-ffe5-e9ffcc5faf27"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1898565, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC-fxo67Drox",
        "outputId": "85ad1be8-ae74-4981-d669-de16542911b9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.01332"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "-fEuYZO7DtI5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "UaggOjKmDvUr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "AyfxOt71Dygn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btulLXTvDz2y",
        "outputId": "ca5e5485-aeab-4afa-f7f3-8e16e7225407"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 28s 126ms/step - loss: 2.7417\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.9987\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 23s 125ms/step - loss: 1.7207\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 23s 126ms/step - loss: 1.5575\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 23s 125ms/step - loss: 1.4556\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 23s 125ms/step - loss: 1.3858\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 23s 125ms/step - loss: 1.3317\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.2865\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.2444\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.2044\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.1650\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.1232\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.0817\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.0350\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 0.9877\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 23s 125ms/step - loss: 0.9362\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 0.8837\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 0.8310\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 0.7790\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 23s 125ms/step - loss: 0.7310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "c0oFAqWAOpMt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "8fHErsntOsEV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peeNSLJJOuBA",
        "outputId": "29f296b6-4d63-47ef-c590-307859b6cf53"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "This is my mother: I have promised!\n",
            "For thou shalt know, we say this night. High, mighty subject,\n",
            "Make quaint her whench, on thee, Kate! O, peace!\n",
            "Thou wast by tongue so tender,\n",
            "With too much reverend mark concemption.\n",
            "Is love the tongue of charity, and I\n",
            "Wolsce from the deft he makes opening more\n",
            "Than thou the measure of a fine,\n",
            "I throw us in't in my boy to mome that evil,\n",
            "Flattering me if you shall see, her die.\n",
            "\n",
            "BRUTUS:\n",
            "Say you so?\n",
            "\n",
            "BAPTISTA:\n",
            "How hast thou to A?fia comes freely cown;\n",
            "For joyful sins of birth, or every ghost of heaven\n",
            "With sorrow canopies acquitfold laugh earls\n",
            "Having it, revenge and frame by counted\n",
            "see your own affect to take all mock to do it, I have heard;\n",
            "My false is to the proud in things.\n",
            "\n",
            "HASTINGS:\n",
            "A book of powerful godd! O, let her excellen\n",
            "Hath in the over with me.\n",
            "\n",
            "MARCIUS:\n",
            "They say it is your grace.\n",
            "\n",
            "MARCIUS:\n",
            "My lord, I would that govern'd him.\n",
            "\n",
            "JULIEt:\n",
            "More worst if thou desperately here brought outton?\n",
            "Gentlemen to serve more foes!\n",
            "Grace till thou kno \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 10.179878234863281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAJvM_JYOuw2",
        "outputId": "690c211f-a718-46ff-cdd0-7821cbd5d46e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThe sudden shore for that woman in the vaward\\nTo speak to reprehender of the pierce,\\nSomething rares by the executioner,\\nA very sick man, whom I expect is thus a stal\\nOf cure after turned with the dam;\\nAnd in this seat of wreck, cries me to fight:\\nAnd whither we have seen them, it in a\\ncursed against us; and, in revenue mildsh I rive;\\nSurnay, but by some seems before him.\\n\\nKING RICHARD II:\\nAh, near? what news? what fite other times?\\nCan this bandy power? Who never calumny\\nThan to set to death, and say it is the more.\\n\\nQUEEN:\\nWho sums it you a fair one art.\\n\\nKING HENRY VI:\\nWhy, Sonage; I will give thee to my glory!\\n\\nLADY CAPULET:\\nWell, give me my consent; there's soothing tongue\\nnog of rebellion from the stones far band,\\nMore looks not by another's bastard Andember: lie, and for a fellow;\\nWhy, Camillo, this contracted buts logging:\\nSome paint is stolen or old by, I saw.\\n\\nKING RICHARD II:\\nI hold it to my drinkards:' for thou wouldst,\\nThou art in closents: if one little grave,\\nAnd qualit\"\n",
            " b\"ROMEO:\\nShe shall.\\n\\nKING RICHARD III:\\nThat she's the child, where you shalt star,\\nI shall not flatter essume is free of bot:\\nWhich, wither' warm them, cannot come; to hear\\nAs is your treasures; and so bright another such\\nAs war. Say it both my master blest\\nFrom touch our own part. To figure he spare,\\nEnforced times, I cannot speak. Have I pood outs,\\nNot like a pilly; and bid him pluck may carch\\nTo raise the extremest wives; such as he, that the most\\nstragber our against obhaids; our country's castle,\\nwhose curvesarish of our office want no other\\nAnd make him kiss.'\\n\\nNurse:\\nWife, man, great-mouth and confuler issue.\\n\\nCAMILLO:\\nWho stand, sir?\\n\\nSABPSTAS:\\nI under-tame, seeing thou hast slander 'em\\n'Gidies a black proud orator, to thy parbling\\nAnd strangeless in all pay for any other's head.\\n\\nNORFOLK:\\nWhy, this is Perdant to my sins: since\\nI have received dead and our people away.\\nConciusion show chance them on their course,\\nIt best apprehended toxtured all tyrant;\\nAnd Antonio is command in the nu\"\n",
            " b\"ROMEO:\\nGood morrow, kinsman!\\n\\nPETER:\\nMarry, she is sware methory,\\nor seeming, or coldives of your clothes!\\nCome, go pling.\\n\\nPEdRUCHIO:\\nWell goards he made, you do not like these fasts\\nHere life and all the poor clothes made them fores:\\nBut, look your heast! co murder; and kept I did;\\nThat fear'd out of my sick and lungs that dam.\\nThat laid what is soberlance: therefore, pent\\nWe stolen and sulphous ase upon thyself,\\nAnd stain and full of old Voise, Petruchio, deal,\\nPurpoue bare and nunshine divine.\\n\\nPERDITA:\\nNo, sir, bethink thee.\\n\\nCORIOLANUS:\\nDid you mean this?\\nCan I do fond that Blarence and Buckingham.\\nNow go have been as charge, do me; I saw him;\\nA place; o' Tybalt, Juliet,\\nDo at the letter stand condemn'd Bolingbroke.\\n\\nQUEEN:\\nNoble fearful of your daughters, poison walk,\\nHumbled by him only son'd.\\n\\nVIRGILIA:\\nBeseech you, sir.\\n\\nESCALUS:\\nHow have you frown in that autyon\\nUpon that Clarence: and my soul-brotherling,\\nGives on exceed that are bestowed her husband\\nAnd to red inquire your lawfu\"\n",
            " b\"ROMEO:\\nHe hath commands that glass'd the sleepher world,\\nAnd so die for itself.\\n\\nCLIFFORD:\\nWhat bidst it well, and go to speak. And, if he call'd upon\\nTo me and that accuset shake.\\n\\nPROSPERO:\\nThou wilt not bid me of these deadly speech\\nWor die return. not after than deadly loss!\\nAway with the roping of his face.\\n\\nGLOUCESTER:\\nIf I should bear account no more informity\\nWhich shall be proud. But march on, call to her!\\n\\nKAThaRD:\\nThe king of roorish of all kindly queen.\\n\\nLADY GREY:\\nTo her. I saw your honours in another;\\nLet that be ready to a pain of time.\\n\\nBISHOP OF CARLISLE:\\nThey say, but mean, I will be wife's fair. Fare you well.\\n\\nBONA:\\nDo you not seem to bring him down? These I prize impaties\\nThat Edward tell my father Yet?\\nWhere have you nor conducting assurance of the less;\\nThen 'twere to marvellous Somertir, so have their nobless\\nEnd in that venom when I may be carred lear so;\\nAnd therefore I shall do remember my knees:\\nWhich friends still true Polixenes,\\nAnd thrive I not, her friends sha\"\n",
            " b\"ROMEO:\\nI saw a messenger on the base fall of a day.\\n\\nBIANCA:\\nGood-morrow, be not can make him go wander; in,\\nSince our consent to him, if thou shalt hear it.\\n\\nBENVOLIO:\\nThis music I among the curses\\nOf that anchers. If it were state, as thou art,\\nWomanins all, and all the wonder drink\\nat Camores.\\nTime, whom these crowns?\\n\\nMENENIUS:\\nWhere is the damnst thou? speak so, come you not, up.\\n\\nDUCHESS OF YORK:\\nThe seams of France chase that hath madness.\\n\\nKING LEWIS XI:\\nRather, good citizens:\\nWe may be quiet, resides accursed you;\\nDeaf friends, make the lodging up be of your cooking,\\nFather, for stript and flattery,\\nYour countenance soon'd such grief cut out of holy\\nUntil I did; go fly to change of royalty.\\nSir-Lance, how fares our loving mine:\\n'Tis time in earnest warrants.\\n\\nPERDITA:\\nI can good disturb.\\n\\nProvost:\\nNo, my good lord!\\n\\nLEONTES:\\nWould I give low to prown till I bring fortune\\nBy dexanding as be not, 'bouthous' 'ards;\\nIf he were created mounting for his spite.\\n\\nBAPTISTA:\\nWhat, didst thou \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 8.295103788375854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRNu5y5lO22R",
        "outputId": "16796d5d-e5ff-4362-babb-454aeadf4aac"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fb4215d31d0>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iahCUpuO4ju",
        "outputId": "86235c2e-52d9-4b24-c5de-54ab851ce6cf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "My lord gor toNa substance of an interessel-on\n",
            "that reason all the creatures I must have, I am adoi\n"
          ]
        }
      ]
    }
  ]
}